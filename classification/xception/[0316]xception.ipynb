{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yukir\\Documents\\GitHub\\Sign_Recognition\\py\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12429037738060100663\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 16797565939088312483\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 9883535296\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 5299630727136419782\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 1175499543468443476\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n",
      "tf 2.2.0\n",
      "keras 2.3.0-tf\n",
      "set_global_determinism(seed=1337) 이거 꼭 해라\n",
      "set_global_determinism(seed=1337) 이거 꼭 해라\n",
      "set_global_determinism(seed=1337) 이거 꼭 해라\n",
      "C:\\Users\\yukir\\Documents\\Monicas_workspace\\Knee\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\Users\\yukir\\Documents\\GitHub\\Sign_Recognition\\py\n",
    "\n",
    "from basic_preprocessing import *\n",
    "from deeplearning_check import *\n",
    "from machine_learning import *\n",
    "\n",
    "gpu_check()\n",
    "set_global_determinism(seed=SEED)\n",
    "\n",
    "%cd C:\\Users\\yukir\\Documents\\Monicas_workspace\\Knee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet152V2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.applications.xception.Xception(\n",
    "    include_top=True, weights='imagenet', input_tensor=None,\n",
    "    input_shape=None, pooling=None, classes=1000,\n",
    "    classifier_activation='softmax'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class = 5\n",
    "\n",
    "# path to kaggle dataset\n",
    "root_path = \"C:\\\\Users\\\\yukir\\\\Documents\\\\Monicas_workspace\\\\NEW_Knee\\\\data\\\\archive\\\\\"\n",
    "\n",
    "# list of folders\n",
    "folder_list = os.listdir(root_path)\n",
    "image_path_list = []\n",
    "label_list = []\n",
    "\n",
    "# for each folder, get the image path and labels\n",
    "for folder in folder_list:\n",
    "    for label in range(n_class):\n",
    "        \n",
    "        # get all the images path inside the current folder\n",
    "        image_list = os.listdir(f\"{root_path}{folder}/{label}\")\n",
    "        # add to the image path list\n",
    "        image_path_list += [ f\"{root_path}{folder}/{label}/\"+ path for path in image_list]\n",
    "        \n",
    "        # add labels to the label list\n",
    "        label_list += [label] * len(image_list)\n",
    "\n",
    "# convert to dataframe\n",
    "df_train_kaggle = pd.DataFrame({\"filepath\" : image_path_list, \"label\": label_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3857\n",
       "2    2578\n",
       "1    1770\n",
       "3    1286\n",
       "4     295\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_kaggle.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9786, 2)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_kaggle.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'count')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train_kaggle.label.value_counts().plot.bar()\n",
    "plt.xlabel(\"label\")\n",
    "plt.ylabel(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data generator object\n",
    "train_aug = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# validation data generator object\n",
    "valid_aug = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9786 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "# create train generator\n",
    "train_generator = train_aug.flow_from_dataframe(\n",
    "dataframe=df_train_kaggle,\n",
    "directory=None,\n",
    "x_col=\"filepath\",\n",
    "y_col=\"label\",\n",
    "batch_size=32,\n",
    "seed=42,\n",
    "shuffle=True,\n",
    "class_mode=\"raw\",\n",
    "target_size=(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yukir\\Documents\\Monicas_workspace\\NEW_Knee\\data\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\Users\\yukir\\Documents\\Monicas_workspace\\NEW_Knee\\data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gdown --id \"1NdDqPK4NLn2aV8ZdF5ilux1sfG6IyebC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\yukir\\Documents\\Monicas_workspace\\NEW...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\yukir\\Documents\\Monicas_workspace\\NEW...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\yukir\\Documents\\Monicas_workspace\\NEW...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\yukir\\Documents\\Monicas_workspace\\NEW...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\yukir\\Documents\\Monicas_workspace\\NEW...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  label\n",
       "0  C:\\Users\\yukir\\Documents\\Monicas_workspace\\NEW...      0\n",
       "1  C:\\Users\\yukir\\Documents\\Monicas_workspace\\NEW...      1\n",
       "2  C:\\Users\\yukir\\Documents\\Monicas_workspace\\NEW...      0\n",
       "3  C:\\Users\\yukir\\Documents\\Monicas_workspace\\NEW...      1\n",
       "4  C:\\Users\\yukir\\Documents\\Monicas_workspace\\NEW...      2"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read Train.csv file which contains image names and labels and preprocess them\n",
    "compi_root_path= \"C:\\\\Users\\\\yukir\\\\Documents\\\\Monicas_workspace\\\\NEW_Knee\\\\data\\\\KneeXray\\\\KneeXray\\\\\"\n",
    "df_val_compi = pd.read_csv(compi_root_path + \"Train.csv\")\n",
    "\n",
    "# add absolute path to the image names\n",
    "df_val_compi[\"filename\"] = df_val_compi.filename.apply(lambda x: compi_root_path+\"train/\" + x)\n",
    "df_val_compi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'count')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# class count of compitition dataset\n",
    "df_val_compi.label.value_counts().plot.bar()\n",
    "plt.xlabel(\"label\")\n",
    "plt.ylabel(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7828 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "# create validation generator\n",
    "valid_generator = valid_aug.flow_from_dataframe( \n",
    "dataframe= df_val_compi,\n",
    "x_col= \"filename\",\n",
    "y_col= \"label\",\n",
    "batch_size= 32,\n",
    "seed= 42,\n",
    "shuffle= True,\n",
    "class_mode= \"raw\",\n",
    "target_size= (224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001,decay=0.0001),\n",
    "                 metrics=[\"acc\"],\n",
    "                 loss= tf.keras.losses.sparse_categorical_crossentropy)\n",
    "\n",
    "# callbacks and checkpoints\n",
    "checkpoint_path = \"xception_best.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "my_callbacks = [\n",
    "               ModelCheckpoint(checkpoint_path,\n",
    "                               monitor = 'val_acc',\n",
    "                               verbose = 1,\n",
    "                               save_weights_only=True,\n",
    "                               save_best_only = True,\n",
    "                               mode=\"max\"),\n",
    "              EarlyStopping(monitor='val_loss',\n",
    "                            patience=5,\n",
    "                            verbose=0),\n",
    "              ReduceLROnPlateau(monitor='val_loss',\n",
    "                                patience=5,\n",
    "                                verbose=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 classes= np.unique(df_train_kaggle.label.values),\n",
    "                                                 y= df_train_kaggle.label.values)\n",
    "class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5074410163339383,\n",
       " 1: 1.1057627118644069,\n",
       " 2: 0.7591931730023274,\n",
       " 3: 1.5219284603421461,\n",
       " 4: 6.63457627118644}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7828 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "# create validation generator\n",
    "valid_generator = valid_aug.flow_from_dataframe( \n",
    "dataframe= df_val_compi,\n",
    "x_col= \"filename\",\n",
    "y_col= \"label\",\n",
    "batch_size= 32,\n",
    "seed= 42,\n",
    "shuffle= True,\n",
    "class_mode= \"raw\",\n",
    "target_size= (224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yukir\\anaconda3\\envs\\tensorflow37\\lib\\site-packages\\gdown\\cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  category=FutureWarning,\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1NdDqPK4NLn2aV8ZdF5ilux1sfG6IyebC\n",
      "To: C:\\Users\\yukir\\Documents\\Monicas_workspace\\NEW_Knee\\data\\KneeXray.zip\n",
      "\n",
      "  0%|          | 0.00/120M [00:00<?, ?B/s]\n",
      "  0%|          | 524k/120M [00:00<01:43, 1.15MB/s]\n",
      "  1%|          | 1.05M/120M [00:00<00:56, 2.11MB/s]\n",
      "  2%|▏         | 2.10M/120M [00:00<00:28, 4.18MB/s]\n",
      "  3%|▎         | 3.15M/120M [00:00<00:26, 4.45MB/s]\n",
      "  4%|▎         | 4.19M/120M [00:01<00:25, 4.44MB/s]\n",
      "  6%|▌         | 6.82M/120M [00:01<00:13, 8.16MB/s]\n",
      "  7%|▋         | 7.86M/120M [00:01<00:12, 8.62MB/s]\n",
      "  8%|▊         | 9.44M/120M [00:01<00:11, 9.49MB/s]\n",
      "  9%|▉         | 11.0M/120M [00:01<00:10, 10.1MB/s]\n",
      " 11%|█         | 12.6M/120M [00:01<00:10, 10.6MB/s]\n",
      " 12%|█▏        | 14.2M/120M [00:01<00:10, 10.3MB/s]\n",
      " 13%|█▎        | 15.7M/120M [00:02<00:09, 10.8MB/s]\n",
      " 14%|█▍        | 17.3M/120M [00:02<00:09, 11.1MB/s]\n",
      " 16%|█▌        | 18.9M/120M [00:02<00:08, 11.3MB/s]\n",
      " 17%|█▋        | 20.4M/120M [00:02<00:10, 9.59MB/s]\n",
      " 19%|█▉        | 23.1M/120M [00:02<00:08, 11.8MB/s]\n",
      " 21%|██        | 24.6M/120M [00:02<00:08, 11.8MB/s]\n",
      " 22%|██▏       | 26.2M/120M [00:02<00:07, 11.8MB/s]\n",
      " 23%|██▎       | 27.8M/120M [00:03<00:09, 9.93MB/s]\n",
      " 25%|██▌       | 30.4M/120M [00:03<00:07, 12.2MB/s]\n",
      " 27%|██▋       | 32.0M/120M [00:03<00:07, 12.1MB/s]\n",
      " 28%|██▊       | 33.6M/120M [00:03<00:07, 12.0MB/s]\n",
      " 29%|██▉       | 35.1M/120M [00:03<00:08, 9.94MB/s]\n",
      " 32%|███▏      | 37.7M/120M [00:03<00:06, 12.5MB/s]\n",
      " 33%|███▎      | 39.3M/120M [00:04<00:06, 12.3MB/s]\n",
      " 34%|███▍      | 40.9M/120M [00:04<00:06, 12.2MB/s]\n",
      " 35%|███▌      | 42.5M/120M [00:04<00:07, 10.1MB/s]\n",
      " 37%|███▋      | 44.0M/120M [00:04<00:07, 9.53MB/s]\n",
      " 38%|███▊      | 45.6M/120M [00:04<00:08, 8.66MB/s]\n",
      " 40%|████      | 48.2M/120M [00:04<00:06, 11.0MB/s]\n",
      " 42%|████▏     | 49.8M/120M [00:05<00:06, 11.2MB/s]\n",
      " 43%|████▎     | 51.4M/120M [00:05<00:06, 11.4MB/s]\n",
      " 44%|████▍     | 53.0M/120M [00:05<00:06, 9.77MB/s]\n",
      " 46%|████▋     | 55.6M/120M [00:05<00:06, 10.2MB/s]\n",
      " 48%|████▊     | 57.1M/120M [00:05<00:06, 10.4MB/s]\n",
      " 49%|████▉     | 58.7M/120M [00:05<00:05, 10.7MB/s]\n",
      " 50%|█████     | 60.3M/120M [00:06<00:06, 9.45MB/s]\n",
      " 53%|█████▎    | 62.9M/120M [00:06<00:04, 11.7MB/s]\n",
      " 54%|█████▍    | 64.5M/120M [00:06<00:05, 9.55MB/s]\n",
      " 55%|█████▌    | 66.1M/120M [00:06<00:05, 10.1MB/s]\n",
      " 57%|█████▋    | 67.6M/120M [00:06<00:05, 10.3MB/s]\n",
      " 58%|█████▊    | 69.2M/120M [00:07<00:04, 10.7MB/s]\n",
      " 59%|█████▉    | 70.8M/120M [00:07<00:04, 11.0MB/s]\n",
      " 60%|██████    | 72.4M/120M [00:07<00:04, 11.2MB/s]\n",
      " 62%|██████▏   | 73.9M/120M [00:07<00:04, 9.92MB/s]\n",
      " 63%|██████▎   | 75.5M/120M [00:07<00:04, 9.82MB/s]\n",
      " 64%|██████▍   | 77.1M/120M [00:07<00:04, 10.3MB/s]\n",
      " 66%|██████▌   | 78.6M/120M [00:07<00:03, 10.8MB/s]\n",
      " 67%|██████▋   | 80.2M/120M [00:08<00:04, 9.52MB/s]\n",
      " 69%|██████▉   | 82.3M/120M [00:08<00:03, 11.6MB/s]\n",
      " 70%|███████   | 83.9M/120M [00:08<00:03, 11.7MB/s]\n",
      " 71%|███████▏  | 85.5M/120M [00:08<00:02, 11.7MB/s]\n",
      " 73%|███████▎  | 87.0M/120M [00:08<00:03, 10.1MB/s]\n",
      " 74%|███████▍  | 89.1M/120M [00:08<00:02, 11.9MB/s]\n",
      " 76%|███████▌  | 90.7M/120M [00:08<00:02, 11.8MB/s]\n",
      " 77%|███████▋  | 92.3M/120M [00:09<00:02, 11.7MB/s]\n",
      " 78%|███████▊  | 93.8M/120M [00:09<00:02, 11.8MB/s]\n",
      " 80%|███████▉  | 95.4M/120M [00:09<00:02, 10.0MB/s]\n",
      " 81%|████████  | 97.0M/120M [00:09<00:02, 9.94MB/s]\n",
      " 82%|████████▏ | 98.6M/120M [00:09<00:02, 10.4MB/s]\n",
      " 84%|████████▎ | 100M/120M [00:09<00:02, 9.26MB/s] \n",
      " 85%|████████▌ | 102M/120M [00:10<00:01, 11.3MB/s]\n",
      " 87%|████████▋ | 104M/120M [00:10<00:01, 11.4MB/s]\n",
      " 88%|████████▊ | 105M/120M [00:10<00:01, 11.5MB/s]\n",
      " 89%|████████▉ | 107M/120M [00:10<00:01, 11.6MB/s]\n",
      " 91%|█████████ | 109M/120M [00:10<00:00, 11.2MB/s]\n",
      " 92%|█████████▏| 110M/120M [00:10<00:00, 11.6MB/s]\n",
      " 93%|█████████▎| 112M/120M [00:10<00:00, 11.4MB/s]\n",
      " 95%|█████████▍| 113M/120M [00:11<00:00, 11.4MB/s]\n",
      " 96%|█████████▌| 115M/120M [00:11<00:00, 8.66MB/s]\n",
      " 97%|█████████▋| 116M/120M [00:11<00:00, 9.28MB/s]\n",
      " 99%|█████████▊| 118M/120M [00:11<00:00, 9.94MB/s]\n",
      "100%|█████████▉| 120M/120M [00:11<00:00, 10.4MB/s]\n",
      "100%|██████████| 120M/120M [00:11<00:00, 10.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306/306 [==============================] - ETA: 0s - loss: 4.1481 - acc: 0.2119\n",
      "Epoch 00001: val_acc improved from -inf to 0.26648, saving model to xception_best.ckpt\n",
      "306/306 [==============================] - 138s 450ms/step - loss: 4.1481 - acc: 0.2119 - val_loss: 2.1382 - val_acc: 0.2665 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 1.4420 - acc: 0.4407\n",
      "Epoch 00002: val_acc improved from 0.26648 to 0.39997, saving model to xception_best.ckpt\n",
      "306/306 [==============================] - 135s 441ms/step - loss: 1.4420 - acc: 0.4407 - val_loss: 1.5087 - val_acc: 0.4000 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 1.0418 - acc: 0.5184\n",
      "Epoch 00003: val_acc improved from 0.39997 to 0.57256, saving model to xception_best.ckpt\n",
      "306/306 [==============================] - 135s 442ms/step - loss: 1.0418 - acc: 0.5184 - val_loss: 1.0806 - val_acc: 0.5726 - lr: 1.0000e-05\n",
      "Epoch 4/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 0.8771 - acc: 0.5695\n",
      "Epoch 00004: val_acc improved from 0.57256 to 0.59057, saving model to xception_best.ckpt\n",
      "306/306 [==============================] - 134s 438ms/step - loss: 0.8771 - acc: 0.5695 - val_loss: 1.0079 - val_acc: 0.5906 - lr: 1.0000e-05\n",
      "Epoch 5/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 0.7506 - acc: 0.6264\n",
      "Epoch 00005: val_acc improved from 0.59057 to 0.65496, saving model to xception_best.ckpt\n",
      "306/306 [==============================] - 133s 435ms/step - loss: 0.7506 - acc: 0.6264 - val_loss: 0.8753 - val_acc: 0.6550 - lr: 1.0000e-05\n",
      "Epoch 6/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 0.6334 - acc: 0.6814\n",
      "Epoch 00006: val_acc improved from 0.65496 to 0.66658, saving model to xception_best.ckpt\n",
      "306/306 [==============================] - 134s 438ms/step - loss: 0.6334 - acc: 0.6814 - val_loss: 0.8378 - val_acc: 0.6666 - lr: 1.0000e-05\n",
      "Epoch 7/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 0.5400 - acc: 0.7333\n",
      "Epoch 00007: val_acc improved from 0.66658 to 0.71002, saving model to xception_best.ckpt\n",
      "306/306 [==============================] - 137s 446ms/step - loss: 0.5400 - acc: 0.7333 - val_loss: 0.7229 - val_acc: 0.7100 - lr: 1.0000e-05\n",
      "Epoch 8/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 0.4459 - acc: 0.7854\n",
      "Epoch 00008: val_acc improved from 0.71002 to 0.76162, saving model to xception_best.ckpt\n",
      "306/306 [==============================] - 139s 456ms/step - loss: 0.4459 - acc: 0.7854 - val_loss: 0.6169 - val_acc: 0.7616 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 0.3575 - acc: 0.8350\n",
      "Epoch 00009: val_acc improved from 0.76162 to 0.77670, saving model to xception_best.ckpt\n",
      "306/306 [==============================] - 141s 460ms/step - loss: 0.3575 - acc: 0.8350 - val_loss: 0.5898 - val_acc: 0.7767 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 0.2701 - acc: 0.8834\n",
      "Epoch 00010: val_acc improved from 0.77670 to 0.82333, saving model to xception_best.ckpt\n",
      "306/306 [==============================] - 140s 456ms/step - loss: 0.2701 - acc: 0.8834 - val_loss: 0.4803 - val_acc: 0.8233 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 0.2083 - acc: 0.9185\n",
      "Epoch 00011: val_acc improved from 0.82333 to 0.82971, saving model to xception_best.ckpt\n",
      "306/306 [==============================] - 141s 459ms/step - loss: 0.2083 - acc: 0.9185 - val_loss: 0.4543 - val_acc: 0.8297 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 0.1434 - acc: 0.9527\n",
      "Epoch 00012: val_acc improved from 0.82971 to 0.86254, saving model to xception_best.ckpt\n",
      "306/306 [==============================] - 135s 441ms/step - loss: 0.1434 - acc: 0.9527 - val_loss: 0.3876 - val_acc: 0.8625 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 0.1070 - acc: 0.9661\n",
      "Epoch 00013: val_acc improved from 0.86254 to 0.87621, saving model to xception_best.ckpt\n",
      "306/306 [==============================] - 134s 439ms/step - loss: 0.1070 - acc: 0.9661 - val_loss: 0.3464 - val_acc: 0.8762 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 0.0739 - acc: 0.9818\n",
      "Epoch 00014: val_acc improved from 0.87621 to 0.87685, saving model to xception_best.ckpt\n",
      "306/306 [==============================] - 132s 432ms/step - loss: 0.0739 - acc: 0.9818 - val_loss: 0.3489 - val_acc: 0.8769 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 0.0586 - acc: 0.9852\n",
      "Epoch 00015: val_acc improved from 0.87685 to 0.89167, saving model to xception_best.ckpt\n",
      "306/306 [==============================] - 135s 441ms/step - loss: 0.0586 - acc: 0.9852 - val_loss: 0.3090 - val_acc: 0.8917 - lr: 1.0000e-05\n",
      "Epoch 16/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 0.0523 - acc: 0.9858\n",
      "Epoch 00016: val_acc improved from 0.89167 to 0.90112, saving model to xception_best.ckpt\n",
      "306/306 [==============================] - 134s 439ms/step - loss: 0.0523 - acc: 0.9858 - val_loss: 0.2863 - val_acc: 0.9011 - lr: 1.0000e-05\n",
      "Epoch 17/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 0.0383 - acc: 0.9912\n",
      "Epoch 00017: val_acc improved from 0.90112 to 0.90278, saving model to xception_best.ckpt\n",
      "306/306 [==============================] - 134s 438ms/step - loss: 0.0383 - acc: 0.9912 - val_loss: 0.2747 - val_acc: 0.9028 - lr: 1.0000e-05\n",
      "Epoch 18/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 0.0347 - acc: 0.9933\n",
      "Epoch 00018: val_acc improved from 0.90278 to 0.91300, saving model to xception_best.ckpt\n",
      "306/306 [==============================] - 134s 437ms/step - loss: 0.0347 - acc: 0.9933 - val_loss: 0.2517 - val_acc: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 19/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 0.0244 - acc: 0.9955\n",
      "Epoch 00019: val_acc did not improve from 0.91300\n",
      "306/306 [==============================] - 139s 454ms/step - loss: 0.0244 - acc: 0.9955 - val_loss: 0.2758 - val_acc: 0.9052 - lr: 1.0000e-05\n",
      "Epoch 20/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 0.0251 - acc: 0.9950\n",
      "Epoch 00020: val_acc improved from 0.91300 to 0.91850, saving model to xception_best.ckpt\n",
      "306/306 [==============================] - 140s 456ms/step - loss: 0.0251 - acc: 0.9950 - val_loss: 0.2292 - val_acc: 0.9185 - lr: 1.0000e-05\n",
      "Epoch 21/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 0.0204 - acc: 0.9958\n",
      "Epoch 00021: val_acc did not improve from 0.91850\n",
      "306/306 [==============================] - 138s 451ms/step - loss: 0.0204 - acc: 0.9958 - val_loss: 0.2362 - val_acc: 0.9176 - lr: 1.0000e-05\n",
      "Epoch 22/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 0.0176 - acc: 0.9965\n",
      "Epoch 00022: val_acc improved from 0.91850 to 0.91875, saving model to xception_best.ckpt\n",
      "306/306 [==============================] - 141s 459ms/step - loss: 0.0176 - acc: 0.9965 - val_loss: 0.2370 - val_acc: 0.9188 - lr: 1.0000e-05\n",
      "Epoch 23/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 0.0158 - acc: 0.9965\n",
      "Epoch 00023: val_acc improved from 0.91875 to 0.92105, saving model to xception_best.ckpt\n",
      "306/306 [==============================] - 139s 454ms/step - loss: 0.0158 - acc: 0.9965 - val_loss: 0.2218 - val_acc: 0.9211 - lr: 1.0000e-05\n",
      "Epoch 24/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 0.0117 - acc: 0.9981\n",
      "Epoch 00024: val_acc improved from 0.92105 to 0.92501, saving model to xception_best.ckpt\n",
      "306/306 [==============================] - 141s 460ms/step - loss: 0.0117 - acc: 0.9981 - val_loss: 0.2127 - val_acc: 0.9250 - lr: 1.0000e-05\n",
      "Epoch 25/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 0.0132 - acc: 0.9966\n",
      "Epoch 00025: val_acc improved from 0.92501 to 0.92540, saving model to xception_best.ckpt\n",
      "306/306 [==============================] - 139s 454ms/step - loss: 0.0132 - acc: 0.9966 - val_loss: 0.2169 - val_acc: 0.9254 - lr: 1.0000e-05\n",
      "Epoch 26/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 0.0126 - acc: 0.9971\n",
      "Epoch 00026: val_acc did not improve from 0.92540\n",
      "306/306 [==============================] - 139s 455ms/step - loss: 0.0126 - acc: 0.9971 - val_loss: 0.2198 - val_acc: 0.9253 - lr: 1.0000e-05\n",
      "Epoch 27/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 0.0118 - acc: 0.9978\n",
      "Epoch 00027: val_acc improved from 0.92540 to 0.92603, saving model to xception_best.ckpt\n",
      "306/306 [==============================] - 139s 456ms/step - loss: 0.0118 - acc: 0.9978 - val_loss: 0.2127 - val_acc: 0.9260 - lr: 1.0000e-05\n",
      "Epoch 28/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 0.0114 - acc: 0.9979\n",
      "Epoch 00028: val_acc improved from 0.92603 to 0.93242, saving model to xception_best.ckpt\n",
      "306/306 [==============================] - 139s 455ms/step - loss: 0.0114 - acc: 0.9979 - val_loss: 0.2029 - val_acc: 0.9324 - lr: 1.0000e-05\n",
      "Epoch 29/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 0.0092 - acc: 0.9979\n",
      "Epoch 00029: val_acc did not improve from 0.93242\n",
      "306/306 [==============================] - 139s 455ms/step - loss: 0.0092 - acc: 0.9979 - val_loss: 0.2068 - val_acc: 0.9299 - lr: 1.0000e-05\n",
      "Epoch 30/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 0.0088 - acc: 0.9976\n",
      "Epoch 00030: val_acc did not improve from 0.93242\n",
      "306/306 [==============================] - 139s 455ms/step - loss: 0.0088 - acc: 0.9976 - val_loss: 0.2378 - val_acc: 0.9186 - lr: 1.0000e-05\n",
      "Epoch 31/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 0.0070 - acc: 0.9988\n",
      "Epoch 00031: val_acc improved from 0.93242 to 0.93344, saving model to xception_best.ckpt\n",
      "306/306 [==============================] - 141s 461ms/step - loss: 0.0070 - acc: 0.9988 - val_loss: 0.1907 - val_acc: 0.9334 - lr: 1.0000e-05\n",
      "Epoch 32/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 0.0078 - acc: 0.9983\n",
      "Epoch 00032: val_acc did not improve from 0.93344\n",
      "306/306 [==============================] - 138s 451ms/step - loss: 0.0078 - acc: 0.9983 - val_loss: 0.2052 - val_acc: 0.9306 - lr: 1.0000e-05\n",
      "Epoch 33/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 0.0067 - acc: 0.9980\n",
      "Epoch 00033: val_acc did not improve from 0.93344\n",
      "306/306 [==============================] - 135s 442ms/step - loss: 0.0067 - acc: 0.9980 - val_loss: 0.1888 - val_acc: 0.9333 - lr: 1.0000e-05\n",
      "Epoch 34/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 0.0080 - acc: 0.9978\n",
      "Epoch 00034: val_acc did not improve from 0.93344\n",
      "306/306 [==============================] - 140s 459ms/step - loss: 0.0080 - acc: 0.9978 - val_loss: 0.2038 - val_acc: 0.9305 - lr: 1.0000e-05\n",
      "Epoch 35/100\n",
      "306/306 [==============================] - ETA: 0s - loss: 0.0058 - acc: 0.9990\n",
      "Epoch 00035: val_acc did not improve from 0.93344\n",
      "306/306 [==============================] - 145s 474ms/step - loss: 0.0058 - acc: 0.9990 - val_loss: 0.2012 - val_acc: 0.9306 - lr: 1.0000e-05\n",
      "Epoch 36/100\n",
      "173/306 [===============>..............] - ETA: 52s - loss: 0.0074 - acc: 0.9978"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8240/46318645.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmy_callbacks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         class_weight=class_weights)\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# load best saved weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    853\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[1;31m# No error, now safe to assign to logs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow37\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    387\u001b[0m     \"\"\"\n\u001b[0;32m    388\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow37\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_process_logs\u001b[1;34m(self, logs)\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;34m\"\"\"Turns tensors into numpy arrays or Python scalars.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow37\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 523\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow37\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 617\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow37\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 617\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow37\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    517\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    520\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow37\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    959\u001b[0m     \"\"\"\n\u001b[0;32m    960\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 961\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    962\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow37\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    925\u001b[0m     \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 927\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    928\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model.fit(\n",
    "        train_generator,\n",
    "        epochs=100,\n",
    "        validation_data=valid_generator,\n",
    "        callbacks=[my_callbacks],\n",
    "        class_weight=class_weights)\n",
    "\n",
    "# load best saved weights\n",
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.evaluate(valid_generator)\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicition_compi = model.predict(valid_generator, steps= valid_generator.n/ BATCH_SIZE, verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "class_prediction_compi =  np.argmax(predicition_compi, axis= 1)\n",
    "cm = confusion_matrix(df_val_compi.label, class_prediction_compi, labels=[0, 1, 2, 3, 4])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=[0, 1, 2, 3, 4])\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xception_model.save_weights(\"knee_xray_ResNet152V2.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compition data로 더 가르키는거\n",
    "https://www.kaggle.com/haidermasood/xception-osteoarthritis-kneexray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validation split on competition data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test = train_test_split(df_train_kaggle,\n",
    "                                   test_size=0.2,\n",
    "                                   random_state=42,\n",
    "                                   stratify= df_train_kaggle.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validation split on competition data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test = train_test_split(df_val_compi,\n",
    "                                   test_size=0.1,\n",
    "                                   random_state=42,\n",
    "                                   stratify= df_val_compi.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_aug.flow_from_dataframe(\n",
    "dataframe = X_train,\n",
    "x_col=\"filepath\",\n",
    "y_col=\"label\",\n",
    "batch_size=32,\n",
    "seed=42,\n",
    "shuffle=True,\n",
    "class_mode=\"raw\",\n",
    "target_size=(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_generator = valid_aug.flow_from_dataframe( \n",
    "dataframe=X_test,\n",
    "x_col=\"filepath\",\n",
    "y_col=\"label\",\n",
    "batch_size=32,\n",
    "seed=42,\n",
    "shuffle=True,\n",
    "class_mode=\"raw\",\n",
    "target_size=(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of steps to consider 1 as  epoch\n",
    "STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID =valid_generator.n//valid_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kick off training\n",
    "xception_model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "        epochs=50,\n",
    "        validation_data=valid_generator,\n",
    "        validation_steps=STEP_SIZE_VALID,callbacks=[my_callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xception_model.save_weights(\"knee_final_xray_ResNet152V2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best saved weights\n",
    "xception_model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Confusion Matirx**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "target_shape = 224\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# test generator\n",
    "compi_gen = valid_aug.flow_from_dataframe(dataframe= X_test,\n",
    "                            x_col= \"filepath\",\n",
    "                            class_mode=None,\n",
    "                            target_size= (target_shape, target_shape),\n",
    "                            shuffle= False,\n",
    "                            batch_size= BATCH_SIZE\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction on train data\n",
    "predicition_compi = model.predict(compi_gen, steps= compi_gen.n/ BATCH_SIZE, verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "class_prediction_compi =  np.argmax(predicition_compi, axis= 1)\n",
    "cm = confusion_matrix(X_test.label, class_prediction_compi, labels=[0, 1, 2, 3, 4])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=[0, 1, 2, 3, 4])\n",
    "disp.plot()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d93d6df27dcc797823232b994e4f43d959f011089850837812bf48ca3e70a46"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('tensorflow37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
